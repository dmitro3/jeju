# Babylon RL Training Configuration
# Supports: Tinker (Cloud), CUDA GPU, Apple Silicon (MLX), CPU

# ==============================================================================
# REQUIRED FOR ALL TRAINING
# ==============================================================================

# Database connection for trajectory data
DATABASE_URL=postgresql://user:password@localhost:5432/babylon

# OpenAI API key for RLAIF judge (GPT-4o-mini)
OPENAI_API_KEY=sk-...

# ==============================================================================
# TINKER CLOUD TRAINING (RECOMMENDED)
# ==============================================================================
# Get your API key from: https://tinker-docs.thinkingmachines.ai/

TINKER_API_KEY=

# Tinker model selection (see Tinker docs for full list)
# TINKER_BASE_MODEL=Qwen/Qwen3-30B-A3B-Instruct
# TINKER_BASE_MODEL=Qwen/Qwen3-235B-A22B-Instruct  # Largest
# TINKER_BASE_MODEL=meta-llama/Llama-3.1-70B

# ==============================================================================
# LOCAL ATROPOS TRAINING (Alternative)
# ==============================================================================
# Only needed if NOT using Tinker

# Atropos API server URL
ATROPOS_API_URL=http://localhost:8000

# vLLM inference server port
VLLM_PORT=9001

# ==============================================================================
# BACKEND SELECTION (Local training only)
# ==============================================================================
# If none set, auto-detects: CUDA > MLX > CPU
# Set ONE of these to true to force a specific backend:

# USE_MLX_BACKEND=true      # Apple Silicon Mac (fastest on Mac)
# USE_LOCAL_BACKEND=true    # CUDA GPU (auto-falls back to MLX/CPU if no CUDA)
# USE_CPU_BACKEND=true      # CPU only (SLOW - not recommended)

# ==============================================================================
# MODEL SELECTION (Local training only)
# ==============================================================================
# Leave blank to auto-select based on backend:
# - Default: unsloth/Qwen3-4B-128K (4B params, 128K context)
# - CUDA: unsloth/Qwen3-4B-128K
# - MLX: mlx-community/Qwen2.5-3B-Instruct-4bit
# - CPU: unsloth/Qwen3-4B-128K

# BASE_MODEL=unsloth/Qwen3-4B-128K
# BASE_MODEL=mlx-community/Qwen2.5-3B-Instruct-4bit
# BASE_MODEL=mlx-community/Qwen2.5-7B-Instruct-4bit

BASE_MODEL=

# ==============================================================================
# TRAINING CONFIGURATION
# ==============================================================================

# Data selection
MIN_AGENTS_PER_WINDOW=1
# WINDOW_ID=              # Specific window to train (optional)
# MAX_EXAMPLES=2000       # Limit trajectories
# MAX_STEPS_PER_TRAJECTORY=20

# Training hyperparameters
# LEARNING_RATE=4e-5      # Tinker default
# LEARNING_RATE=1e-5      # Atropos default
# MAX_SEQ_LENGTH=4096     # Context length
# LORA_RANK=32            # LoRA rank (Tinker)
# GROUP_SIZE=4            # GRPO group size
# TRAINING_STEPS=100      # Number of steps

# ==============================================================================
# JUDGE MODEL (RLAIF Scoring)
# ==============================================================================

JUDGE_MODEL=gpt-4o-mini
# JUDGE_MODEL=gpt-4o    # Higher quality, higher cost
# JUDGE_TEMPERATURE=0.3
